{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"NLP/README..../","text":"nlp-datasets \u00b6 github.com Apache Software Foundation Public Mail Archives : all publicly available Apache Software Foundation mail archives as of July 11, 2011 (200 GB) Blog Authorship Corpus : consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. 681,288 posts and over 140 million words. (298 MB) Amazon Fine Food Reviews [Kaggle] : consists of 568,454 food reviews Amazon users left up to October 2012. Paper . (240 MB) Amazon Reviews : Stanford collection of 35 million amazon reviews. (11 GB) ArXiv : All the Papers on archive as fulltext (270 GB) + sourcefiles (190 GB). CLiPS Stylometry Investigation (CSI) Corpus : a yearly expanded corpus of student texts in two genres: essays and reviews. The purpose of this corpus lies primarily in stylometric research, but other applications are possible. (on request) ClueWeb09 FACC : ClueWeb09 with Freebase annotations (72 GB) ClueWeb11 FACC : ClueWeb11 with Freebase annotations (92 GB) Common Crawl Corpus : web crawl data composed of over 5 billion web pages (541 TB) Cornell Movie Dialog Corpus : contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: 220,579 conversational exchanges between 10,292 pairs of movie characters, 617 movies (9.5 MB) Crosswikis : English-phrase-to-associated-Wikipedia-article database. Paper. (11 GB) DBpedia : a community effort to extract structured information from Wikipedia and to make this information available on the Web (17 GB) Death Row : last words of every inmate executed since 1984 online (HTML table) Diplomacy : 17,000 conversational messages from 12 games of Diplomacy, annotated for truthfulness (3 MB) Elsevier OA CC-BY Corpus : 40k (40,001) Open Access full-text scientific articles with complete metadata include subject classifications (963Mb) Enron Email Data : consists of 1,227,255 emails with 493,384 attachments covering 151 custodians (210 GB) Event Registry : Free tool that gives real time access to news articles by 100.000 news publishers worldwide. Has API . (query tool) Examiner.com - Spam Clickbait News Headlines [Kaggle] : 3 Million crowdsourced News headlines published by now defunct clickbait website The Examiner from 2010 to 2015. (200 MB) Federal Contracts from the Federal Procurement Data Center (USASpending.gov) : data dump of all federal contracts from the Federal Procurement Data Center found at USASpending.gov (180 GB) Flickr Personal Taxonomies : Tree dataset of personal tags (40 MB) Freebase Data Dump : data dump of all the current facts and assertions in Freebase (26 GB) Freebase Simple Topic Dump : data dump of the basic identifying facts about every topic in Freebase (5 GB) Freebase Quad Dump : data dump of all the current facts and assertions in Freebase (35 GB) GigaOM Wordpress Challenge [Kaggle] : blog posts, meta data, user likes (1.5 GB) Google Books Ngrams : available also in hadoop format on amazon s3 (2.2 TB) Google Web 5gram : contains English word n-grams and their observed frequency counts (24 GB) Gutenberg Ebook List : annotated list of ebooks (2 MB) Gutenberg Standardized Corpus : Standardized Project Gutenberg Corpus, 55905 books (3GB counts + 18GB tokens) Hansards text chunks of Canadian Parliament : 1.3 million pairs of aligned text chunks (sentences or smaller fragments) from the official records (Hansards) of the 36th Canadian Parliament. (82 MB) Harvard Library : over 12 million bibliographic records for materials held by the Harvard Library, including books, journals, electronic resources, manuscripts, archival materials, scores, audio, video and other materials. (4 GB) Hate speech identification : Contributors viewed short text and identified if it a) contained hate speech, b) was offensive but without hate speech, or c) was not offensive at all. Contains nearly 15K rows with three contributor judgments per text string. (3 MB) Hillary Clinton Emails [Kaggle] : nearly 7,000 pages of Clinton\u2019s heavily redacted emails (12 MB) Historical Newspapers Yearly N-grams and Entities Dataset : Yearly time series for the usage of the 1,000,000 most frequent 1-, 2-, and 3-grams from a subset of the British Newspaper Archive corpus, along with yearly time series for the 100,000 most frequent named entities linked to Wikipedia and a list of all articles and newspapers contained in the dataset (3.1 GB) Historical Newspapers Daily Word Time Series Dataset : Time series of daily word usage for the 25,000 most frequent words in 87 years of UK and US historical newspapers between 1836 and 1922. (2.7GB) Home Depot Product Search Relevance [Kaggle] : contains a number of products and real customer search terms from Home Depot\u2019s website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters. (65 MB) Identifying key phrases in text : Question/Answer pairs + context; context was judged if relevant to question/answer. (8 MB) Jeopardy : archive of 216,930 past Jeopardy questions (53 MB) 200k English plaintext jokes : archive of 208,000 plaintext jokes from various sources. Machine Translation of European Languages : (612 MB) Material Safety Datasheets : 230,000 Material Safety Data Sheets. (3 GB) Million News Headlines - ABC Australia [Kaggle] : 1.3 Million News headlines published by ABC News Australia from 2003 to 2017. (56 MB) Millions of News Article URLs : 2.3 million URLs for news articles from the frontpage of over 950 English-language news outlets in the six month period between October 2014 and April 2015. (101MB) News Headlines of India - Times of India [Kaggle] : 2.7 Million News Headlines with category published by Times of India from 2001 to 2017. (185 MB) News article / Wikipedia page pairings : Contributors read a short article and were asked which of two Wikipedia articles it matched most closely. (6 MB) NIPS2015 Papers (version 2) [Kaggle] : full text of all NIPS2015 papers (335 MB) NYTimes Facebook Data : all the NYTimes facebook posts (5 MB) One Week of Global News Feeds [Kaggle] : News Event Dataset of 1.4 Million Articles published globally in 20 languages over one week of August 2017. (115 MB) Objective truths of sentences/concept pairs : Contributors read a sentence with two concepts. For example \u201ca dog is a kind of animal\u201d or \u201ccaptain can have the same meaning as master.\u201d They were then asked if the sentence could be true and ranked it on a 1-5 scale. (700 KB) Open Library Data Dumps : dump of all revisions of all the records in Open Library. (16 GB) Personae Corpus : collected for experiments in Authorship Attribution and Personality Prediction. It consists of 145 Dutch-language essays by 145 different students. (on request) Reddit Comments : every publicly available reddit comment as of july 2015. 1.7 billion comments (250 GB) Reddit Comments (May \u201815) [Kaggle] : subset of above dataset (8 GB) Reddit Submission Corpus : all publicly available Reddit submissions from January 2006 - August 31, 2015). (42 GB) Reuters Corpus : a large collection of Reuters News stories for use in research and development of natural language processing, information retrieval, and machine learning systems. This corpus, known as \u201cReuters Corpus, Volume 1\u201d or RCV1, is significantly larger than the older, well-known Reuters-21578 collection heavily used in the text classification community. Need to sign agreement and sent per post to obtain. (2.5 GB) SMS Spam Collection : 5,574 English, real and non-enconded SMS messages, tagged according being legitimate (ham) or spam. (200 KB) SouthparkData : .csv files containing script information including: season, episode, character, & line. (3.6 MB) Stanford Question Answering Dataset (SQUAD 2.0) : a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Stackoverflow : 7.3 million stackoverflow questions + other stackexchanges (query tool) Twitter Cheng-Caverlee-Lee Scrape : Tweets from September 2009 - January 2010, geolocated. (400 MB) Twitter New England Patriots Deflategate sentiment : Before the 2015 Super Bowl, there was a great deal of chatter around deflated footballs and whether the Patriots cheated. This data set looks at Twitter sentiment on important days during the scandal to gauge public sentiment about the whole ordeal. (2 MB) Twitter Progressive issues sentiment analysis : tweets regarding a variety of left-leaning issues like legalization of abortion, feminism, Hillary Clinton, etc. classified if the tweets in question were for, against, or neutral on the issue (with an option for none of the above). (600 KB) Twitter Sentiment140 : Tweets related to brands/keywords. Website includes papers and research ideas. (77 MB) Twitter sentiment analysis: Self-driving cars : contributors read tweets and classified them as very positive, slightly positive, neutral, slightly negative, or very negative. They were also prompted asked to mark if the tweet was not relevant to self-driving cars. (1 MB) Twitter Elections Integrity : All suspicious tweets and media from 2016 US election. (1.4 GB) Twitter Tokyo Geolocated Tweets : 200K tweets from Tokyo. (47 MB) Twitter UK Geolocated Tweets : 170K tweets from UK. (47 MB) Twitter USA Geolocated Tweets : 200k tweets from the US (45MB) Twitter US Airline Sentiment [Kaggle] : A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \u201clate flight\u201d or \u201crude service\u201d). (2.5 MB) U.S. economic performance based on news articles : News articles headlines and excerpts ranked as whether relevant to U.S. economy. (5 MB) Urban Dictionary Words and Definitions [Kaggle] : Cleaned CSV corpus of 2.6 Million of all Urban Dictionary words, definitions, authors, votes as of May 2016. (238 MB) Wesbury Lab Usenet Corpus : anonymized compilation of postings from 47,860 English-language newsgroups from 2005-2010 (40 GB) Wesbury Lab Wikipedia Corpus Snapshot of all the articles in the English part of the Wikipedia that was taken in April 2010. It was processed, as described in detail below, to remove all links and irrelevant material (navigation text, etc) The corpus is untagged, raw text. Used by Stanford NLP (1.8 GB). WorldTree Corpus of Explanation Graphs for Elementary Science Questions : a corpus of manually-constructed explanation graphs, explanatory role ratings, and associated semistructured tablestore for most publicly available elementary science exam questions in the US (8 MB) Wikipedia Extraction (WEX) : a processed dump of english language wikipedia (66 GB) Wikipedia XML Data : complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. (500 GB) Yahoo! Answers Comprehensive Questions and Answers : Yahoo! Answers corpus as of 10/25/2007. Contains 4,483,032 questions and their answers. (3.6 GB) Yahoo! Answers consisting of questions asked in French : Subset of the Yahoo! Answers corpus from 2006 to 2015 consisting of 1.7 million questions posed in French, and their corresponding answers. (3.8 GB) Yahoo! Answers Manner Questions : subset of the Yahoo! Answers corpus from a 10/25/2007 dump, selected for their linguistic properties. Contains 142,627 questions and their answers. (104 MB) Yahoo! HTML Forms Extracted from Publicly Available Webpages : contains a small sample of pages that contain complex HTML forms, contains 2.67 million complex forms. (50+ GB) Yahoo! Metadata Extracted from Publicly Available Web Pages : 100 million triples of RDF data (2 GB) Yahoo N-Gram Representations : This dataset contains n-gram representations. The data may serve as a testbed for query rewriting task, a common problem in IR research as well as to word and sentence similarity task, which is common in NLP research. (2.6 GB) Yahoo! N-Grams, version 2.0 : n-grams (n = 1 to 5), extracted from a corpus of 14.6 million documents (126 million unique sentences, 3.4 billion running words) crawled from over 12000 news-oriented sites (12 GB) Yahoo! Search Logs with Relevance Judgments : Annonymized Yahoo! Search Logs with Relevance Judgments (1.3 GB) Yahoo! Semantically Annotated Snapshot of the English Wikipedia : English Wikipedia dated from 2006-11-04 processed with a number of publicly-available NLP tools. 1,490,688 entries. (6 GB) Yelp : including restaurant rankings and 2.2M reviews (on request) Youtube : 1.7 million youtube videos descriptions (torrent) \u67e5\u770b\u539f\u7f51\u9875: github.com","title":"nlp-datasets"},{"location":"NLP/README..../#nlp-datasets","text":"github.com Apache Software Foundation Public Mail Archives : all publicly available Apache Software Foundation mail archives as of July 11, 2011 (200 GB) Blog Authorship Corpus : consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. 681,288 posts and over 140 million words. (298 MB) Amazon Fine Food Reviews [Kaggle] : consists of 568,454 food reviews Amazon users left up to October 2012. Paper . (240 MB) Amazon Reviews : Stanford collection of 35 million amazon reviews. (11 GB) ArXiv : All the Papers on archive as fulltext (270 GB) + sourcefiles (190 GB). CLiPS Stylometry Investigation (CSI) Corpus : a yearly expanded corpus of student texts in two genres: essays and reviews. The purpose of this corpus lies primarily in stylometric research, but other applications are possible. (on request) ClueWeb09 FACC : ClueWeb09 with Freebase annotations (72 GB) ClueWeb11 FACC : ClueWeb11 with Freebase annotations (92 GB) Common Crawl Corpus : web crawl data composed of over 5 billion web pages (541 TB) Cornell Movie Dialog Corpus : contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: 220,579 conversational exchanges between 10,292 pairs of movie characters, 617 movies (9.5 MB) Crosswikis : English-phrase-to-associated-Wikipedia-article database. Paper. (11 GB) DBpedia : a community effort to extract structured information from Wikipedia and to make this information available on the Web (17 GB) Death Row : last words of every inmate executed since 1984 online (HTML table) Diplomacy : 17,000 conversational messages from 12 games of Diplomacy, annotated for truthfulness (3 MB) Elsevier OA CC-BY Corpus : 40k (40,001) Open Access full-text scientific articles with complete metadata include subject classifications (963Mb) Enron Email Data : consists of 1,227,255 emails with 493,384 attachments covering 151 custodians (210 GB) Event Registry : Free tool that gives real time access to news articles by 100.000 news publishers worldwide. Has API . (query tool) Examiner.com - Spam Clickbait News Headlines [Kaggle] : 3 Million crowdsourced News headlines published by now defunct clickbait website The Examiner from 2010 to 2015. (200 MB) Federal Contracts from the Federal Procurement Data Center (USASpending.gov) : data dump of all federal contracts from the Federal Procurement Data Center found at USASpending.gov (180 GB) Flickr Personal Taxonomies : Tree dataset of personal tags (40 MB) Freebase Data Dump : data dump of all the current facts and assertions in Freebase (26 GB) Freebase Simple Topic Dump : data dump of the basic identifying facts about every topic in Freebase (5 GB) Freebase Quad Dump : data dump of all the current facts and assertions in Freebase (35 GB) GigaOM Wordpress Challenge [Kaggle] : blog posts, meta data, user likes (1.5 GB) Google Books Ngrams : available also in hadoop format on amazon s3 (2.2 TB) Google Web 5gram : contains English word n-grams and their observed frequency counts (24 GB) Gutenberg Ebook List : annotated list of ebooks (2 MB) Gutenberg Standardized Corpus : Standardized Project Gutenberg Corpus, 55905 books (3GB counts + 18GB tokens) Hansards text chunks of Canadian Parliament : 1.3 million pairs of aligned text chunks (sentences or smaller fragments) from the official records (Hansards) of the 36th Canadian Parliament. (82 MB) Harvard Library : over 12 million bibliographic records for materials held by the Harvard Library, including books, journals, electronic resources, manuscripts, archival materials, scores, audio, video and other materials. (4 GB) Hate speech identification : Contributors viewed short text and identified if it a) contained hate speech, b) was offensive but without hate speech, or c) was not offensive at all. Contains nearly 15K rows with three contributor judgments per text string. (3 MB) Hillary Clinton Emails [Kaggle] : nearly 7,000 pages of Clinton\u2019s heavily redacted emails (12 MB) Historical Newspapers Yearly N-grams and Entities Dataset : Yearly time series for the usage of the 1,000,000 most frequent 1-, 2-, and 3-grams from a subset of the British Newspaper Archive corpus, along with yearly time series for the 100,000 most frequent named entities linked to Wikipedia and a list of all articles and newspapers contained in the dataset (3.1 GB) Historical Newspapers Daily Word Time Series Dataset : Time series of daily word usage for the 25,000 most frequent words in 87 years of UK and US historical newspapers between 1836 and 1922. (2.7GB) Home Depot Product Search Relevance [Kaggle] : contains a number of products and real customer search terms from Home Depot\u2019s website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters. (65 MB) Identifying key phrases in text : Question/Answer pairs + context; context was judged if relevant to question/answer. (8 MB) Jeopardy : archive of 216,930 past Jeopardy questions (53 MB) 200k English plaintext jokes : archive of 208,000 plaintext jokes from various sources. Machine Translation of European Languages : (612 MB) Material Safety Datasheets : 230,000 Material Safety Data Sheets. (3 GB) Million News Headlines - ABC Australia [Kaggle] : 1.3 Million News headlines published by ABC News Australia from 2003 to 2017. (56 MB) Millions of News Article URLs : 2.3 million URLs for news articles from the frontpage of over 950 English-language news outlets in the six month period between October 2014 and April 2015. (101MB) News Headlines of India - Times of India [Kaggle] : 2.7 Million News Headlines with category published by Times of India from 2001 to 2017. (185 MB) News article / Wikipedia page pairings : Contributors read a short article and were asked which of two Wikipedia articles it matched most closely. (6 MB) NIPS2015 Papers (version 2) [Kaggle] : full text of all NIPS2015 papers (335 MB) NYTimes Facebook Data : all the NYTimes facebook posts (5 MB) One Week of Global News Feeds [Kaggle] : News Event Dataset of 1.4 Million Articles published globally in 20 languages over one week of August 2017. (115 MB) Objective truths of sentences/concept pairs : Contributors read a sentence with two concepts. For example \u201ca dog is a kind of animal\u201d or \u201ccaptain can have the same meaning as master.\u201d They were then asked if the sentence could be true and ranked it on a 1-5 scale. (700 KB) Open Library Data Dumps : dump of all revisions of all the records in Open Library. (16 GB) Personae Corpus : collected for experiments in Authorship Attribution and Personality Prediction. It consists of 145 Dutch-language essays by 145 different students. (on request) Reddit Comments : every publicly available reddit comment as of july 2015. 1.7 billion comments (250 GB) Reddit Comments (May \u201815) [Kaggle] : subset of above dataset (8 GB) Reddit Submission Corpus : all publicly available Reddit submissions from January 2006 - August 31, 2015). (42 GB) Reuters Corpus : a large collection of Reuters News stories for use in research and development of natural language processing, information retrieval, and machine learning systems. This corpus, known as \u201cReuters Corpus, Volume 1\u201d or RCV1, is significantly larger than the older, well-known Reuters-21578 collection heavily used in the text classification community. Need to sign agreement and sent per post to obtain. (2.5 GB) SMS Spam Collection : 5,574 English, real and non-enconded SMS messages, tagged according being legitimate (ham) or spam. (200 KB) SouthparkData : .csv files containing script information including: season, episode, character, & line. (3.6 MB) Stanford Question Answering Dataset (SQUAD 2.0) : a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. Stackoverflow : 7.3 million stackoverflow questions + other stackexchanges (query tool) Twitter Cheng-Caverlee-Lee Scrape : Tweets from September 2009 - January 2010, geolocated. (400 MB) Twitter New England Patriots Deflategate sentiment : Before the 2015 Super Bowl, there was a great deal of chatter around deflated footballs and whether the Patriots cheated. This data set looks at Twitter sentiment on important days during the scandal to gauge public sentiment about the whole ordeal. (2 MB) Twitter Progressive issues sentiment analysis : tweets regarding a variety of left-leaning issues like legalization of abortion, feminism, Hillary Clinton, etc. classified if the tweets in question were for, against, or neutral on the issue (with an option for none of the above). (600 KB) Twitter Sentiment140 : Tweets related to brands/keywords. Website includes papers and research ideas. (77 MB) Twitter sentiment analysis: Self-driving cars : contributors read tweets and classified them as very positive, slightly positive, neutral, slightly negative, or very negative. They were also prompted asked to mark if the tweet was not relevant to self-driving cars. (1 MB) Twitter Elections Integrity : All suspicious tweets and media from 2016 US election. (1.4 GB) Twitter Tokyo Geolocated Tweets : 200K tweets from Tokyo. (47 MB) Twitter UK Geolocated Tweets : 170K tweets from UK. (47 MB) Twitter USA Geolocated Tweets : 200k tweets from the US (45MB) Twitter US Airline Sentiment [Kaggle] : A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \u201clate flight\u201d or \u201crude service\u201d). (2.5 MB) U.S. economic performance based on news articles : News articles headlines and excerpts ranked as whether relevant to U.S. economy. (5 MB) Urban Dictionary Words and Definitions [Kaggle] : Cleaned CSV corpus of 2.6 Million of all Urban Dictionary words, definitions, authors, votes as of May 2016. (238 MB) Wesbury Lab Usenet Corpus : anonymized compilation of postings from 47,860 English-language newsgroups from 2005-2010 (40 GB) Wesbury Lab Wikipedia Corpus Snapshot of all the articles in the English part of the Wikipedia that was taken in April 2010. It was processed, as described in detail below, to remove all links and irrelevant material (navigation text, etc) The corpus is untagged, raw text. Used by Stanford NLP (1.8 GB). WorldTree Corpus of Explanation Graphs for Elementary Science Questions : a corpus of manually-constructed explanation graphs, explanatory role ratings, and associated semistructured tablestore for most publicly available elementary science exam questions in the US (8 MB) Wikipedia Extraction (WEX) : a processed dump of english language wikipedia (66 GB) Wikipedia XML Data : complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. (500 GB) Yahoo! Answers Comprehensive Questions and Answers : Yahoo! Answers corpus as of 10/25/2007. Contains 4,483,032 questions and their answers. (3.6 GB) Yahoo! Answers consisting of questions asked in French : Subset of the Yahoo! Answers corpus from 2006 to 2015 consisting of 1.7 million questions posed in French, and their corresponding answers. (3.8 GB) Yahoo! Answers Manner Questions : subset of the Yahoo! Answers corpus from a 10/25/2007 dump, selected for their linguistic properties. Contains 142,627 questions and their answers. (104 MB) Yahoo! HTML Forms Extracted from Publicly Available Webpages : contains a small sample of pages that contain complex HTML forms, contains 2.67 million complex forms. (50+ GB) Yahoo! Metadata Extracted from Publicly Available Web Pages : 100 million triples of RDF data (2 GB) Yahoo N-Gram Representations : This dataset contains n-gram representations. The data may serve as a testbed for query rewriting task, a common problem in IR research as well as to word and sentence similarity task, which is common in NLP research. (2.6 GB) Yahoo! N-Grams, version 2.0 : n-grams (n = 1 to 5), extracted from a corpus of 14.6 million documents (126 million unique sentences, 3.4 billion running words) crawled from over 12000 news-oriented sites (12 GB) Yahoo! Search Logs with Relevance Judgments : Annonymized Yahoo! Search Logs with Relevance Judgments (1.3 GB) Yahoo! Semantically Annotated Snapshot of the English Wikipedia : English Wikipedia dated from 2006-11-04 processed with a number of publicly-available NLP tools. 1,490,688 entries. (6 GB) Yelp : including restaurant rankings and 2.2M reviews (on request) Youtube : 1.7 million youtube videos descriptions (torrent) \u67e5\u770b\u539f\u7f51\u9875: github.com","title":"nlp-datasets"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/","text":"Reddit\u722c\u866b\u7b14\u8bb0 \u00b6 Reddit is a network of communities where people can dive into their interests, hobbies and passions. There\u2019s a community for whatever you\u2019re interested in on Reddit . \u7b80\u5355\u6765\u8bb2\uff0cReddit\u5c31\u662f\u5728\u7279\u5b9a\u5b50\u4e3b\u9898\u4e0b\u7528\u6237\u81ea\u52a8\u53d1\u6398\u5185\u5bb9\u548c\u5206\u4eab\u7684\u8ba8\u8bba\u793e\u533a\uff0c\u5176\u4e2d\u5305\u542b\u5e16\u5b50\u3001\u8bc4\u8bba\u3001\u6295\u7968\u3001\u70b9\u8d5e\u3001\u5206\u4eab\u7b49\u64cd\u4f5c\uff0c\u5176\u64cd\u4f5c\u81ea\u7531\u5ea6\u548c\u4ea4\u4e92\u6027\u975e\u5e38\u9002\u5408\u5e7f\u5927\u7528\u6237\u521b\u9020\u4ef7\u503c\u3002 Reddit\u5bf9\u4e8e\u6570\u636e\u6316\u6398\u5de5\u4f5c\u8005\u975e\u5e38\u53cb\u597d\u3002\u5176\u4e00\u5728\u4e8e\uff0c\u76ee\u524d\u5df2\u6709\u975e\u5e38\u591a\u7684Reddit\u516c\u5f00\u6570\u636e\u96c6\u53ef\u4ee5\u65b9\u4fbf\u83b7\u53d6\uff0c\u5982REDDITBINARY\u3001REDDITMULTI5K\u7b49\u6570\u636e\u96c6\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9876\u4f1a\u4e2d\u7cfb\u5217\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u53e6\u5916\uff0cReddit\u7f51\u7ad9\u4e5f\u63d0\u4f9b\u4e86\u9650\u5236\u8f83\u5c11\u7684API\uff0c\u53ef\u4ee5\u7b80\u5355\u4e0a\u624b\u83b7\u53d6\u5230\u79f0\u5fc3\u5982\u610f\u7684\u4e00\u624b\u7814\u7a76\u6570\u636e\u3002\u56e0\u6b64\uff0c\u7814\u7a76Reddit\u6570\u636e\u96c6\u5982\u4f55\u83b7\u53d6\u5bf9\u4e8e\u5f80\u540e\u7684\u7814\u7a76\u5de5\u4f5c\u662f\u5341\u5206\u5fc5\u8981\u7684\u3002 Reddit\u722c\u866b\u65b9\u5f0f \u00b6 Reddit API \u00b6 \u6709\u7528\u7684\u94fe\u63a5\u5730\u5740 \u5b98\u65b9 api \u8bf4\u660e\u6587\u6863 \u5b98\u65b9 api \u4f7f\u7528\u89c4\u8303 reddit \u6388\u6743\u8bf4\u660e reddit app \u5217\u8868\u5730\u5740 praw \u6587\u6863 python \u91c7\u96c6 reddit \u4f8b\u5b50 \u5177\u4f53\u64cd\u4f5c\u6b65\u9aa4\uff1a \u6ce8\u518cReddit\u8d26\u53f7 \u7533\u8bf7Reddit API \u9996\u5148\u5728 Reddit app \u4ee5Script for personal use\u8eab\u4efd\u586b\u5199name, description, redirect url\uff0c\u7136\u540ecreate app\u5f97\u5230\u4ee5\u4e0b\u7684\u4fe1\u606f\uff1a client_id : The client ID is at least a 14-character string listed just under \u201cpersonal use script\u201d for the desired developed application client_secret : The client secret is at least a 27-character string listed adjacent to secret for the application. password : The password for the Reddit account used to register the application. username : The username of the Reddit account used to register the application. \u4fe1\u606f\u8be6\u7ec6\u4ecb\u7ecd\u8bf7\u770b\uff1a Authenticating via OAuth \u2014 PRAW 7.6.1.dev0 documentation \u5229\u7528\u4ee5\u4e0a\u4fe1\u606f\u5f00\u59cb\u586b\u5199\u4ee3\u7801 # coding: UTF-8 #!/use/bin/env python3 import praw import pandas as pd import datetime as dt reddit = praw.Reddit( client_id='your-clientID', client_secret='your secret', user_agent='your_platform:dev_tmp:v0.1 (by /u/dev_tmp)', # redirect_uri='http://localhost:8080' username='dev_tmp', password='HGFhgf123' ) # print(reddit.auth.url([\"identity\"], \"...\", \"permanent\")) print(reddit.user.me()) # all \u662f<class 'praw.models.reddit.subreddit.Subreddit'>\u7c7b\u578b, # \u5177\u4f53\u4f7f\u7528\u89c1\uff1ahttps://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html all = reddit.subreddit(\"all\") print(type(all)) # submission \u7684\u7c7b\u578b\u662f<class 'praw.models.reddit.submission.Submission'>\uff0c # \u5177\u4f53\u5c5e\u6027\u5217\u8868\u89c1\uff1ahttps://praw.readthedocs.io/en/latest/code_overview/models/submission.html messages = { \"id\": [], \"url\": [], \"title\": [], \"score\": [], \"comms_num\": [], \"body\": [], \"created\": [] } for submission in all.search(\"tiktok\", limit=5): messages[\"id\"].append(submission.id) messages[\"url\"].append(submission.url) messages[\"title\"].append(submission.title) messages[\"score\"].append(submission.score) messages[\"comms_num\"].append(submission.num_comments) messages[\"body\"].append(submission.selftext) messages[\"created\"].append(submission.created) # search \u7ed3\u679c\u7c7b\u578b\u662f<class 'praw.models.listing.generator.ListingGenerator'> # praw \u4e2d\u7684\u5176\u4ed6\u7c7b\u578b\u6587\u6863\u4e3a\uff1a https://praw.readthedocs.io/en/latest/code_overview/other.html # res = all.search(\"tiktok\") # print(type(res)) data = pd.DataFrame(messages) data.to_csv('data.csv', index=False) \u8fd9\u6837\u5c31\u80fd\u83b7\u5f97\u4f60\u60f3\u8981\u7684\u6570\u636e\u4e86 reddit API\u6301\u7eed\u5f00\u53d1| \u8fd9\u90e8\u5206\u53ef\u4ee5\u8be6\u89c1Reddit\u7ed9\u51fa\u7684\u5b98\u65b9\u6587\u6863\uff0c\u6309\u7167\u6587\u6863\u5373\u53ef\u5f97\u5230\u76f8\u5e94\u7684\u6570\u636e\u3002 \u4f46\u662f\u53ef\u80fd\u4f1a\u5b58\u5728\u4ee5\u4e0b\u7684\u95ee\u9898\uff0c\u6211\u5df2\u7ecf\u7ed9\u51fa\u4e86\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b\u3002 \u95ee\u9898\uff1a\u76ee\u524dReddit \u53ea\u80fd\u79d1\u5b66\u8bbf\u95ee\u3002\u5982\u679c\u6309\u7167\u6b63\u5e38\u7684\u722c\u866b\u65b9\u5f0f\u6765\u83b7\u53d6\u7f51\u7ad9\u6570\u636e\u4f1a\u9047\u5230 prawcore.exceptions.RequestException: error with request HTTPSConnectionPool(host=\u2019 www.reddit.com \u2019, port=443 ): Max retries exceeded with url: /api/v1/access_token (Caused by ProxyError(\u2018Cannot connect to proxy.\u2019, OSError(0, \u2018Error\u2019)))>) \u89e3\u51b3: \u76f8\u5173\u89e3\u51b3\u53c2\u8003 # HTTPSConnectionPool(host=\u2019xxxxx\u2019, port=443): Max retries exceeded with url:xxxxxxxx (Caused by Ne\u2026 , python\u722c\u866b\u4e4brequests.exceptions.ProxyError: HTTPSConnectionPool(host=\u2019www.xxxx.com\u2019, port=443): Max retries exceeded with url: / (Caused by ProxyError(\u2018Cannot connect to proxy.\u2019, timeout(\u2018_ssl.c:1108: Th - Eeyhan - \u535a\u5ba2\u56ed (cnblogs.com) - \u4f7f\u7528VPN\u79d1\u5b66\u8bbf\u95ee\u89e3\u51b3error with request HTTPSConnectionPool\u3002l[Clash for Windows](../Clash%20for%20Windows \"Clash for Windows\") \u4ec5\u652f\u6301\u6d4f\u89c8\u5668\u7f51\u9875\uff0c\u5176\u4ed6\u7a0b\u5e8f\u7c7b\u8f6f\u4ef6\u65e0\u6cd5\u4f7f\u7528\u5b83\u8fdb\u884c\u79d1\u5b66\u4e0a\u7f51\u3002\u4f46\u662f\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u4fbf\u6377\u7684\u5de5\u5177TUN Mode\uff0c\u53ef\u4ee5\u4f7f\u6240\u6709\u7684\u7a0b\u5e8f\u90fd\u7ecf\u8fc7Proxy\u8fdb\u884c\u79d1\u5b66\u8bbf\u95ee\uff0c\u8fd9\u6837\u5c31\u80fd\u89e3\u51b3ProxyError\u7b49\u95ee\u9898\u3002\u53e6\u5916\uff0c\u6700\u597d\u9009\u53d6GLobal\u4e0b\u7684\u7f8e\u56fd\u670d\u52a1\u5668\u4f1a\u66f4\u52a0\u7a33\u5b9a\u3002\u5982\u679c\u60f3\u66f4\u52a0\u9ad8\u6548\u7a33\u5b9a\u722c\u53d6\u5927\u91cfReddit\u6570\u636e\uff0c\u53ef\u4ee5\u9009\u62e9\u5c06[\u7a0b\u5e8f\u90e8\u7f72\u56fd\u5916\u5916\u7f51\u670d\u52a1\u5668\u722c\u53d6](../%E7%A8%8B%E5%BA%8F%E9%83%A8%E7%BD%B2%E5%9B%BD%E5%A4%96%E5%A4%96%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%88%AC%E5%8F%96 \"\u7a0b\u5e8f\u90e8\u7f72\u56fd\u5916\u5916\u7f51\u670d\u52a1\u5668\u722c\u53d6\")\u3002 - \u5173\u95ed\u7cfb\u7edf\u4ee3\u7406ProxyError\u3002\u7cfb\u7edf\u4ee3\u7406\u4ecd\u7136\u4f1a\u5bfc\u81f4\u51fa\u73b0\u4e0a\u8ff0\u9519\u8bef\u3002 ![Pasted image 20220520174235.png](../assets/img/Pasted%20image%2020220520174235.png \"Pasted image 20220520174235.png\") Reddit\u7f51\u7ad9API\u722c\u866b\u793a\u4f8b\u53ef\u4ee5\u53c2\u89c1\uff1a (36\u6761\u6d88\u606f) Reddit\u7f51\u7ad9\u83b7\u8d5e\u6700\u9ad8\u6587\u7ae0/\u8bc4\u8bba\u7684\u722c\u53d6_\u5c0f\u4e2b\u5934\u3044\u7684\u535a\u5ba2-CSDN\u535a\u5ba2 \u5982\u4f55\u91c7\u96c6reddit - hgf doing - OSCHINA - \u4e2d\u6587\u5f00\u6e90\u6280\u672f\u4ea4\u6d41\u793e\u533a Scrapy \u722c\u866b \u00b6 Scrapy\u722c\u866b\u7684\u7b80\u5355\u4ecb\u7ecd\u53ef\u4ee5\u53c2\u89c1\uff1a 1. Scrapy\u6846\u67b6\u4ecb\u7ecd\u4e0e\u5b89\u88c5 \u00b7 GitBook (csdn.net) Scrapy\u722c\u866b\u6846\u67b6\u6559\u7a0b\uff08\u4e00\uff09\u2013 Scrapy\u5165\u95e8 - \u77e5\u4e4e (zhihu.com) (36\u6761\u6d88\u606f) Scrapy\u722c\u866b\u5165\u95e8\u6559\u7a0b\u5341\u4e09 Settings\uff08\u8bbe\u7f6e\uff09___\u9759\u7985__\u7684\u535a\u5ba2-CSDN\u535a\u5ba2 \u521b\u5efa\u9879\u76ee scrapy startproject reddit \u5c31\u4f1a\u5f97\u5230\u8fd9\u6837\u7684\u6587\u4ef6\u5939 reddit/ scrapy.cfg reddit/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... \u8fd9\u4e9b\u6587\u4ef6\u5206\u522b\u662f: scrapy.cfg: \u9879\u76ee\u7684\u914d\u7f6e\u6587\u4ef6\u3002 reddit/: \u8be5\u9879\u76ee\u7684python\u6a21\u5757\u3002\u4e4b\u540e\u60a8\u5c06\u5728\u6b64\u52a0\u5165\u4ee3\u7801\u3002 reddit/items.py: \u9879\u76ee\u4e2d\u7684item\u6587\u4ef6\u3002 reddit/pipelines.py: \u9879\u76ee\u4e2d\u7684pipelines\u6587\u4ef6\u3002 reddit/settings.py: \u9879\u76ee\u7684\u8bbe\u7f6e\u6587\u4ef6\u3002 reddit/spiders/: \u653e\u7f6espider\u4ee3\u7801\u7684\u76ee\u5f55\u3002 \u5728reddit/spiders\u76ee\u5f55\u4e0b\u521b\u5efareddit.py\u6587\u4ef6 scrapy genspider reddit reddit.com \u542f\u52a8\u722c\u866b scrapy crawl reddit \u8fd9\u6837\u5c31\u53ef\u4ee5\u722c\u53d6reddit.com\u4e0a\u7684\u5b50\u793e\u533a\u7684\u6570\u636e\u5566\u3002 Reddit\u7f51\u7ad9Scrapy\u722c\u866b\u793a\u4f8b\u53ef\u4ee5\u53c2\u89c1\uff1a Article-for-Datartisan/\u57fa\u4e8eScrapy\u7684Reddit\u722c\u53d6\u4e0e\u5206\u6790.md at master \u00b7 fibears/Article-for-Datartisan (github.com) Scraping Reddit - DataScienceCentral.com StrikingLoo/kitten-getter: A Spider that crawls reddit.com/r/cats (github.com) \u4f2a\u88c5\u8bf7\u6c42\u5934\u722c\u866b \u00b6 request\u7f51\u9875\u8fd4\u56deresponse import requests import csv import time from bs4 import BeautifulSoup4 url = \"https://old.reddit.com/r/datascience/\" # Headers to mimic a browser visit headers = {'User-Agent': 'Mozilla/5.0'} # Returns a requests.models.Response object page = requests.get(url, headers=headers) \u9700\u8981\u4e3b\u8981\u662f\u8fd9\u6837\u7684\u65b9\u5f0f\u722c\u866b\u4f1a\u5bfc\u81f4UA\u548cIP\u88ab\u5c01\uff0c\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5efa\u7acbRandom UA\u548cIP\u6c60\u6765\u52a8\u6001\u7ba1\u7406\u722c\u866b\u8fdb\u7a0b\u7684\u4f2a\u88c5\uff0c\u5177\u4f53\u7684\u64cd\u4f5c\u53ef\u4ee5\u8be6\u89c1\uff1a (36\u6761\u6d88\u606f) \u516b\u3001python\u722c\u866b\u4f2a\u88c5 [\u514d\u8d39\u4f2a\u88c5ip\u4f2a\u88c5\u8bf7\u6c42\u5934]_\u8881\u516d\u52a0.\u7684\u535a\u5ba2-CSDN\u535a\u5ba2_python\u4f2a\u88c5ip Reddit\u7f51\u7ad9\u4f2a\u88c5\u8bf7\u6c42\u5934\u722c\u866b\u793a\u4f8b\u53ef\u4ee5\u53c2\u89c1\uff1a \u4f7f\u7528Python\u548cBeautifulSoup 4\u6293\u53d6Reddit - H5W3 \u603b\u7ed3 \u00b6 \u81f3\u6b64\uff0c\u5173\u4e8eReddit\u7f51\u7ad9\u7684\u722c\u866b\u65b9\u5f0f\u5df2\u7ecf\u4ecb\u7ecd\u5b8c\u6bd5\uff0c\u540e\u7eed\u7684\u4f7f\u7528\u5c06\u7ee7\u7eed\u66f4\u65b0\u3002","title":"Reddit\u722c\u866b\u7b14\u8bb0"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/#reddit","text":"Reddit is a network of communities where people can dive into their interests, hobbies and passions. There\u2019s a community for whatever you\u2019re interested in on Reddit . \u7b80\u5355\u6765\u8bb2\uff0cReddit\u5c31\u662f\u5728\u7279\u5b9a\u5b50\u4e3b\u9898\u4e0b\u7528\u6237\u81ea\u52a8\u53d1\u6398\u5185\u5bb9\u548c\u5206\u4eab\u7684\u8ba8\u8bba\u793e\u533a\uff0c\u5176\u4e2d\u5305\u542b\u5e16\u5b50\u3001\u8bc4\u8bba\u3001\u6295\u7968\u3001\u70b9\u8d5e\u3001\u5206\u4eab\u7b49\u64cd\u4f5c\uff0c\u5176\u64cd\u4f5c\u81ea\u7531\u5ea6\u548c\u4ea4\u4e92\u6027\u975e\u5e38\u9002\u5408\u5e7f\u5927\u7528\u6237\u521b\u9020\u4ef7\u503c\u3002 Reddit\u5bf9\u4e8e\u6570\u636e\u6316\u6398\u5de5\u4f5c\u8005\u975e\u5e38\u53cb\u597d\u3002\u5176\u4e00\u5728\u4e8e\uff0c\u76ee\u524d\u5df2\u6709\u975e\u5e38\u591a\u7684Reddit\u516c\u5f00\u6570\u636e\u96c6\u53ef\u4ee5\u65b9\u4fbf\u83b7\u53d6\uff0c\u5982REDDITBINARY\u3001REDDITMULTI5K\u7b49\u6570\u636e\u96c6\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9876\u4f1a\u4e2d\u7cfb\u5217\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u53e6\u5916\uff0cReddit\u7f51\u7ad9\u4e5f\u63d0\u4f9b\u4e86\u9650\u5236\u8f83\u5c11\u7684API\uff0c\u53ef\u4ee5\u7b80\u5355\u4e0a\u624b\u83b7\u53d6\u5230\u79f0\u5fc3\u5982\u610f\u7684\u4e00\u624b\u7814\u7a76\u6570\u636e\u3002\u56e0\u6b64\uff0c\u7814\u7a76Reddit\u6570\u636e\u96c6\u5982\u4f55\u83b7\u53d6\u5bf9\u4e8e\u5f80\u540e\u7684\u7814\u7a76\u5de5\u4f5c\u662f\u5341\u5206\u5fc5\u8981\u7684\u3002","title":"Reddit\u722c\u866b\u7b14\u8bb0"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/#reddit_1","text":"","title":"Reddit\u722c\u866b\u65b9\u5f0f"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/#reddit-api","text":"\u6709\u7528\u7684\u94fe\u63a5\u5730\u5740 \u5b98\u65b9 api \u8bf4\u660e\u6587\u6863 \u5b98\u65b9 api \u4f7f\u7528\u89c4\u8303 reddit \u6388\u6743\u8bf4\u660e reddit app \u5217\u8868\u5730\u5740 praw \u6587\u6863 python \u91c7\u96c6 reddit \u4f8b\u5b50 \u5177\u4f53\u64cd\u4f5c\u6b65\u9aa4\uff1a \u6ce8\u518cReddit\u8d26\u53f7 \u7533\u8bf7Reddit API \u9996\u5148\u5728 Reddit app \u4ee5Script for personal use\u8eab\u4efd\u586b\u5199name, description, redirect url\uff0c\u7136\u540ecreate app\u5f97\u5230\u4ee5\u4e0b\u7684\u4fe1\u606f\uff1a client_id : The client ID is at least a 14-character string listed just under \u201cpersonal use script\u201d for the desired developed application client_secret : The client secret is at least a 27-character string listed adjacent to secret for the application. password : The password for the Reddit account used to register the application. username : The username of the Reddit account used to register the application. \u4fe1\u606f\u8be6\u7ec6\u4ecb\u7ecd\u8bf7\u770b\uff1a Authenticating via OAuth \u2014 PRAW 7.6.1.dev0 documentation \u5229\u7528\u4ee5\u4e0a\u4fe1\u606f\u5f00\u59cb\u586b\u5199\u4ee3\u7801 # coding: UTF-8 #!/use/bin/env python3 import praw import pandas as pd import datetime as dt reddit = praw.Reddit( client_id='your-clientID', client_secret='your secret', user_agent='your_platform:dev_tmp:v0.1 (by /u/dev_tmp)', # redirect_uri='http://localhost:8080' username='dev_tmp', password='HGFhgf123' ) # print(reddit.auth.url([\"identity\"], \"...\", \"permanent\")) print(reddit.user.me()) # all \u662f<class 'praw.models.reddit.subreddit.Subreddit'>\u7c7b\u578b, # \u5177\u4f53\u4f7f\u7528\u89c1\uff1ahttps://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html all = reddit.subreddit(\"all\") print(type(all)) # submission \u7684\u7c7b\u578b\u662f<class 'praw.models.reddit.submission.Submission'>\uff0c # \u5177\u4f53\u5c5e\u6027\u5217\u8868\u89c1\uff1ahttps://praw.readthedocs.io/en/latest/code_overview/models/submission.html messages = { \"id\": [], \"url\": [], \"title\": [], \"score\": [], \"comms_num\": [], \"body\": [], \"created\": [] } for submission in all.search(\"tiktok\", limit=5): messages[\"id\"].append(submission.id) messages[\"url\"].append(submission.url) messages[\"title\"].append(submission.title) messages[\"score\"].append(submission.score) messages[\"comms_num\"].append(submission.num_comments) messages[\"body\"].append(submission.selftext) messages[\"created\"].append(submission.created) # search \u7ed3\u679c\u7c7b\u578b\u662f<class 'praw.models.listing.generator.ListingGenerator'> # praw \u4e2d\u7684\u5176\u4ed6\u7c7b\u578b\u6587\u6863\u4e3a\uff1a https://praw.readthedocs.io/en/latest/code_overview/other.html # res = all.search(\"tiktok\") # print(type(res)) data = pd.DataFrame(messages) data.to_csv('data.csv', index=False) \u8fd9\u6837\u5c31\u80fd\u83b7\u5f97\u4f60\u60f3\u8981\u7684\u6570\u636e\u4e86 reddit API\u6301\u7eed\u5f00\u53d1| \u8fd9\u90e8\u5206\u53ef\u4ee5\u8be6\u89c1Reddit\u7ed9\u51fa\u7684\u5b98\u65b9\u6587\u6863\uff0c\u6309\u7167\u6587\u6863\u5373\u53ef\u5f97\u5230\u76f8\u5e94\u7684\u6570\u636e\u3002 \u4f46\u662f\u53ef\u80fd\u4f1a\u5b58\u5728\u4ee5\u4e0b\u7684\u95ee\u9898\uff0c\u6211\u5df2\u7ecf\u7ed9\u51fa\u4e86\u89e3\u51b3\u65b9\u6cd5\u5982\u4e0b\u3002 \u95ee\u9898\uff1a\u76ee\u524dReddit \u53ea\u80fd\u79d1\u5b66\u8bbf\u95ee\u3002\u5982\u679c\u6309\u7167\u6b63\u5e38\u7684\u722c\u866b\u65b9\u5f0f\u6765\u83b7\u53d6\u7f51\u7ad9\u6570\u636e\u4f1a\u9047\u5230 prawcore.exceptions.RequestException: error with request HTTPSConnectionPool(host=\u2019 www.reddit.com \u2019, port=443 ): Max retries exceeded with url: /api/v1/access_token (Caused by ProxyError(\u2018Cannot connect to proxy.\u2019, OSError(0, \u2018Error\u2019)))>) \u89e3\u51b3: \u76f8\u5173\u89e3\u51b3\u53c2\u8003 # HTTPSConnectionPool(host=\u2019xxxxx\u2019, port=443): Max retries exceeded with url:xxxxxxxx (Caused by Ne\u2026 , python\u722c\u866b\u4e4brequests.exceptions.ProxyError: HTTPSConnectionPool(host=\u2019www.xxxx.com\u2019, port=443): Max retries exceeded with url: / (Caused by ProxyError(\u2018Cannot connect to proxy.\u2019, timeout(\u2018_ssl.c:1108: Th - Eeyhan - \u535a\u5ba2\u56ed (cnblogs.com) - \u4f7f\u7528VPN\u79d1\u5b66\u8bbf\u95ee\u89e3\u51b3error with request HTTPSConnectionPool\u3002l[Clash for Windows](../Clash%20for%20Windows \"Clash for Windows\") \u4ec5\u652f\u6301\u6d4f\u89c8\u5668\u7f51\u9875\uff0c\u5176\u4ed6\u7a0b\u5e8f\u7c7b\u8f6f\u4ef6\u65e0\u6cd5\u4f7f\u7528\u5b83\u8fdb\u884c\u79d1\u5b66\u4e0a\u7f51\u3002\u4f46\u662f\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u4fbf\u6377\u7684\u5de5\u5177TUN Mode\uff0c\u53ef\u4ee5\u4f7f\u6240\u6709\u7684\u7a0b\u5e8f\u90fd\u7ecf\u8fc7Proxy\u8fdb\u884c\u79d1\u5b66\u8bbf\u95ee\uff0c\u8fd9\u6837\u5c31\u80fd\u89e3\u51b3ProxyError\u7b49\u95ee\u9898\u3002\u53e6\u5916\uff0c\u6700\u597d\u9009\u53d6GLobal\u4e0b\u7684\u7f8e\u56fd\u670d\u52a1\u5668\u4f1a\u66f4\u52a0\u7a33\u5b9a\u3002\u5982\u679c\u60f3\u66f4\u52a0\u9ad8\u6548\u7a33\u5b9a\u722c\u53d6\u5927\u91cfReddit\u6570\u636e\uff0c\u53ef\u4ee5\u9009\u62e9\u5c06[\u7a0b\u5e8f\u90e8\u7f72\u56fd\u5916\u5916\u7f51\u670d\u52a1\u5668\u722c\u53d6](../%E7%A8%8B%E5%BA%8F%E9%83%A8%E7%BD%B2%E5%9B%BD%E5%A4%96%E5%A4%96%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%88%AC%E5%8F%96 \"\u7a0b\u5e8f\u90e8\u7f72\u56fd\u5916\u5916\u7f51\u670d\u52a1\u5668\u722c\u53d6\")\u3002 - \u5173\u95ed\u7cfb\u7edf\u4ee3\u7406ProxyError\u3002\u7cfb\u7edf\u4ee3\u7406\u4ecd\u7136\u4f1a\u5bfc\u81f4\u51fa\u73b0\u4e0a\u8ff0\u9519\u8bef\u3002 ![Pasted image 20220520174235.png](../assets/img/Pasted%20image%2020220520174235.png \"Pasted image 20220520174235.png\") Reddit\u7f51\u7ad9API\u722c\u866b\u793a\u4f8b\u53ef\u4ee5\u53c2\u89c1\uff1a (36\u6761\u6d88\u606f) Reddit\u7f51\u7ad9\u83b7\u8d5e\u6700\u9ad8\u6587\u7ae0/\u8bc4\u8bba\u7684\u722c\u53d6_\u5c0f\u4e2b\u5934\u3044\u7684\u535a\u5ba2-CSDN\u535a\u5ba2 \u5982\u4f55\u91c7\u96c6reddit - hgf doing - OSCHINA - \u4e2d\u6587\u5f00\u6e90\u6280\u672f\u4ea4\u6d41\u793e\u533a","title":"Reddit API"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/#scrapy","text":"Scrapy\u722c\u866b\u7684\u7b80\u5355\u4ecb\u7ecd\u53ef\u4ee5\u53c2\u89c1\uff1a 1. Scrapy\u6846\u67b6\u4ecb\u7ecd\u4e0e\u5b89\u88c5 \u00b7 GitBook (csdn.net) Scrapy\u722c\u866b\u6846\u67b6\u6559\u7a0b\uff08\u4e00\uff09\u2013 Scrapy\u5165\u95e8 - \u77e5\u4e4e (zhihu.com) (36\u6761\u6d88\u606f) Scrapy\u722c\u866b\u5165\u95e8\u6559\u7a0b\u5341\u4e09 Settings\uff08\u8bbe\u7f6e\uff09___\u9759\u7985__\u7684\u535a\u5ba2-CSDN\u535a\u5ba2 \u521b\u5efa\u9879\u76ee scrapy startproject reddit \u5c31\u4f1a\u5f97\u5230\u8fd9\u6837\u7684\u6587\u4ef6\u5939 reddit/ scrapy.cfg reddit/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... \u8fd9\u4e9b\u6587\u4ef6\u5206\u522b\u662f: scrapy.cfg: \u9879\u76ee\u7684\u914d\u7f6e\u6587\u4ef6\u3002 reddit/: \u8be5\u9879\u76ee\u7684python\u6a21\u5757\u3002\u4e4b\u540e\u60a8\u5c06\u5728\u6b64\u52a0\u5165\u4ee3\u7801\u3002 reddit/items.py: \u9879\u76ee\u4e2d\u7684item\u6587\u4ef6\u3002 reddit/pipelines.py: \u9879\u76ee\u4e2d\u7684pipelines\u6587\u4ef6\u3002 reddit/settings.py: \u9879\u76ee\u7684\u8bbe\u7f6e\u6587\u4ef6\u3002 reddit/spiders/: \u653e\u7f6espider\u4ee3\u7801\u7684\u76ee\u5f55\u3002 \u5728reddit/spiders\u76ee\u5f55\u4e0b\u521b\u5efareddit.py\u6587\u4ef6 scrapy genspider reddit reddit.com \u542f\u52a8\u722c\u866b scrapy crawl reddit \u8fd9\u6837\u5c31\u53ef\u4ee5\u722c\u53d6reddit.com\u4e0a\u7684\u5b50\u793e\u533a\u7684\u6570\u636e\u5566\u3002 Reddit\u7f51\u7ad9Scrapy\u722c\u866b\u793a\u4f8b\u53ef\u4ee5\u53c2\u89c1\uff1a Article-for-Datartisan/\u57fa\u4e8eScrapy\u7684Reddit\u722c\u53d6\u4e0e\u5206\u6790.md at master \u00b7 fibears/Article-for-Datartisan (github.com) Scraping Reddit - DataScienceCentral.com StrikingLoo/kitten-getter: A Spider that crawls reddit.com/r/cats (github.com)","title":"Scrapy \u722c\u866b"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/#_1","text":"request\u7f51\u9875\u8fd4\u56deresponse import requests import csv import time from bs4 import BeautifulSoup4 url = \"https://old.reddit.com/r/datascience/\" # Headers to mimic a browser visit headers = {'User-Agent': 'Mozilla/5.0'} # Returns a requests.models.Response object page = requests.get(url, headers=headers) \u9700\u8981\u4e3b\u8981\u662f\u8fd9\u6837\u7684\u65b9\u5f0f\u722c\u866b\u4f1a\u5bfc\u81f4UA\u548cIP\u88ab\u5c01\uff0c\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5efa\u7acbRandom UA\u548cIP\u6c60\u6765\u52a8\u6001\u7ba1\u7406\u722c\u866b\u8fdb\u7a0b\u7684\u4f2a\u88c5\uff0c\u5177\u4f53\u7684\u64cd\u4f5c\u53ef\u4ee5\u8be6\u89c1\uff1a (36\u6761\u6d88\u606f) \u516b\u3001python\u722c\u866b\u4f2a\u88c5 [\u514d\u8d39\u4f2a\u88c5ip\u4f2a\u88c5\u8bf7\u6c42\u5934]_\u8881\u516d\u52a0.\u7684\u535a\u5ba2-CSDN\u535a\u5ba2_python\u4f2a\u88c5ip Reddit\u7f51\u7ad9\u4f2a\u88c5\u8bf7\u6c42\u5934\u722c\u866b\u793a\u4f8b\u53ef\u4ee5\u53c2\u89c1\uff1a \u4f7f\u7528Python\u548cBeautifulSoup 4\u6293\u53d6Reddit - H5W3","title":"\u4f2a\u88c5\u8bf7\u6c42\u5934\u722c\u866b"},{"location":"data%20mining/Reddit%E7%88%AC%E8%99%AB%E6%89%80%E9%81%87%E9%97%AE%E9%A2%98%E8%AF%A6%E8%A7%A3/#_2","text":"\u81f3\u6b64\uff0c\u5173\u4e8eReddit\u7f51\u7ad9\u7684\u722c\u866b\u65b9\u5f0f\u5df2\u7ecf\u4ecb\u7ecd\u5b8c\u6bd5\uff0c\u540e\u7eed\u7684\u4f7f\u7528\u5c06\u7ee7\u7eed\u66f4\u65b0\u3002","title":"\u603b\u7ed3"}]}